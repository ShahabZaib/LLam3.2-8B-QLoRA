## ğŸ§  Simple Guide to Load LLaMA 3.2 with QLoRA

This notebook provides a **minimal and effective setup** to load **Meta's LLaMA 3.2â€“8B** using **QLoRA** (4-bit quantization). No modifications are made â€” it's the original model, fully configurable for your own use cases.

---

### âœ… Instructions

1. **Create or log in** to your [Hugging Face](https://huggingface.co/) account  
2. **Generate a Hugging Face Access Token**  
   â†’ Go to: `https://huggingface.co/settings/tokens`  
3. Paste the token into the code (where prompted) or use secure loading (see â€œToken Securityâ€ below)  
4. Run the notebook to load the model with **4-bit quantization (QLoRA)**

---

### ğŸ’» System Requirements

- QLoRA uses **quantized 4-bit loading**, which **only works on Linux-based systems**
- You can:
  - âœ… Use **native Linux** (recommended for speed)
  - âœ… Or run via **WSL Ubuntu** on Windows (slower but works)

---

### ğŸ” Token Security

**Never hardcode your Hugging Face token in public notebooks.**  
Use one of the following methods:

```bash
# Recommended CLI method
pip install huggingface_hub
huggingface-cli login
